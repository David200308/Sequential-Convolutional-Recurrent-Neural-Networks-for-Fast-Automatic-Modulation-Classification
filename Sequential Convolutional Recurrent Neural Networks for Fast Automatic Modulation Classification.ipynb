{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Convolutional Recurrent Neural Networks for Fast Automatic Modulation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![models](models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filepath=\"RML2016.10a_dict.dat\", test_size=0.1):\n",
    "    \"\"\"\n",
    "    load benchmark dataset.\n",
    "    \"\"\"\n",
    "    radio_data = pickle.load(open(filepath, \"rb\"),\n",
    "                             encoding='latin1')\n",
    "\n",
    "    # ['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
    "    # [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "    mods, snrs = map(lambda idx: sorted(\n",
    "        list(set(map(lambda key: key[idx], radio_data.keys())))), [0, 1])\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for mod in mods:\n",
    "        for snr in snrs:\n",
    "            iqs = radio_data[(mod, snr)]\n",
    "            features.append(iqs)\n",
    "            for i in range(len(iqs)):\n",
    "                labels.append((mod, snr))\n",
    "\n",
    "    X = np.vstack(features)\n",
    "    # Convert X to（n_samples, timesteps, n_features）\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    lb = LabelBinarizer()\n",
    "    y = lb.fit_transform(labels[:, 0])\n",
    "    print(\"Labels: {}\".format(lb.classes_))\n",
    "\n",
    "    X_train, X_test, y_train, y_test, labels_train, labels_test = train_test_split(\n",
    "        X, y, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, labels_train, labels_test\n",
    "\n",
    "def L2_norm(X):\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i,:,0] = X[i,:,0] / np.linalg.norm(X[i,:,0])\n",
    "        \n",
    "    return X\n",
    "\n",
    "def to_amp_phase(X):\n",
    "    X_complex = X[:, :, 0] + 1j * X[:, :, 1]\n",
    "    \n",
    "    X_amp = np.abs(X_complex)\n",
    "    X_ang = np.arctan2(X[:, :, 1], X[:, :, 0]) / np.pi\n",
    "    \n",
    "    \n",
    "    X_amp = X_amp.reshape(X_amp.shape[0], X_amp.shape[1], 1)\n",
    "    X_ang = X_ang.reshape(X_ang.shape[0], X_ang.shape[1], 1)\n",
    "    \n",
    "    X = np.concatenate((X_amp, X_ang), axis=2)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def feature_scaling(X_train, X_test):\n",
    "    scalor = StandardScaler()\n",
    "\n",
    "    X_train_2d = X_train.reshape(\n",
    "        X_train.shape[0]*X_train.shape[1], X_train.shape[2])\n",
    "    X_test_2d = X_test.reshape(\n",
    "        X_test.shape[0]*X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "    scalor.fit(X_train_2d)\n",
    "    X_train_2d_norm = scalor.transform(X_train_2d)\n",
    "    X_test_2d_norm = scalor.transform(X_test_2d)\n",
    "\n",
    "    X_train_norm = X_train_2d_norm.reshape(\n",
    "        X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_test_norm = X_test_2d_norm.reshape(\n",
    "        X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "    return X_train_norm, X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, labels_train, labels_test = load_data(\"RML2016.10a_dict.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = feature_scaling(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The two models are chosen as the baselines for further com- parisons due to their results showing the significant improve- ments upon expert feature-based approaches. Any further improvements should be considered state-of-the-art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One is the CNN architecture proposed by O’shea et al. [19]. As shown in Fig. 1(a), the baseline model is a 4-layer network made up of two convolutional layers and two dense layers. Each hidden layer utilizes rectified linear unit (ReLU) activation functions and dropout of 50% except for a softmax activation function on the one-hot output layer. Adam optimizer and categorical cross entropy loss function are applied to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[2], X_train.shape[1])\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[2], X_test.shape[1])\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(256, (1, 3), activation='relu', padding='same', data_format='channels_first', input_shape=[1, 2, 128]))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Conv2D(80, (2, 3), activation='relu', padding='same', data_format='channels_first', dilation_rate=1))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(11, activation='softmax', kernel_initializer='he_normal'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(X_train_cnn, y_train,\n",
    "                        batch_size=1024,\n",
    "                        epochs=200,\n",
    "                        callbacks=callbacks_list, \n",
    "                        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snrs = [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "\n",
    "metrics_test = []\n",
    "\n",
    "for snr in snrs:\n",
    "    idx = np.where(labels_test[:, 1] == str(snr))\n",
    "    mods = ['AM-SSB', 'PAM4', 'QPSK', '8PSK', 'BPSK', 'QAM16', 'QAM64', 'WBFM', 'CPFSK', 'AM-DSB', 'GFSK']\n",
    "    \n",
    "    loss_and_metrics = model.evaluate(X_test_cnn[idx], y_test[idx], batch_size=128);\n",
    "    metrics_test.append((snr, loss_and_metrics[0], loss_and_metrics[1]))\n",
    "\n",
    "df_metrics_test = pd.DataFrame(metrics_test, columns=['snr', 'loss', 'acc'])\n",
    "df_metrics_test.plot(kind='scatter', x='snr', y='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The other baseline model is proposed by Rajendran et al. [23], shown in Fig. 1(b). The model is comprised of two 128-unit long short-term memory (LSTM) layers and an 11-unit dense layer with a softmax activation. The first LSTM layer returns the full sequences while the sec- ond one just returns the last state. The dropout is also adopted to reduce overfitting. Adam optimizer and categorical cross entropy loss function are applied to the model. Note that this model learns from the time domain information of the modulation schemes using amplitude-phase format, instead of IQ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lstm = to_amp_phase(X_train)\n",
    "X_test_lstm = to_amp_phase(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(128, dropout=0, recurrent_dropout=0.2, return_sequences=True, input_shape=(128, 2)))\n",
    "model.add(keras.layers.LSTM(128, dropout=0, recurrent_dropout=0.2, return_sequences=False))\n",
    "model.add(keras.layers.Dense(11, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(X_train_lstm, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=80,\n",
    "                    callbacks=callbacks_list, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snrs = [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "\n",
    "metrics_test = []\n",
    "\n",
    "for snr in snrs:\n",
    "    idx = np.where(labels_test[:, 1] == str(snr))\n",
    "    mods = ['AM-SSB', 'PAM4', 'QPSK', '8PSK', 'BPSK', 'QAM16', 'QAM64', 'WBFM', 'CPFSK', 'AM-DSB', 'GFSK']\n",
    "    \n",
    "    loss_and_metrics = model.evaluate(X_test_lstm[idx], y_test[idx], batch_size=128);\n",
    "    metrics_test.append((snr, loss_and_metrics[0], loss_and_metrics[1]))\n",
    "\n",
    "df_metrics_test = pd.DataFrame(metrics_test, columns=['snr', 'loss', 'acc'])\n",
    "df_metrics_test.plot(kind='scatter', x='snr', y='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fig. 1(c) provides the illustration of the proposed SCRNN architecture. As schematically shown in Fig. 1(c), the first and second convolutional layers each contain 128 5-tap filters except for the first one followed by a max-pooling layer with a pooling size of 3. The layer 3 and layer 4 are LSTM layers composed of 128 units each, and both return the full sequences. The last dense layer contains 11-class neurons representing the modulation schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu', kernel_constraint=keras.constraints.max_norm(3), input_shape=(128, 2)))  \n",
    "model.add(keras.layers.MaxPooling1D(3))\n",
    "model.add(keras.layers.Conv1D(128, 5, activation='relu', kernel_constraint=keras.constraints.max_norm(3)))  \n",
    "model.add(keras.layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5, return_sequences=True))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(11, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=80,\n",
    "                    callbacks=callbacks_list, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snrs = [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "\n",
    "metrics_test = []\n",
    "\n",
    "for snr in snrs:\n",
    "    idx = np.where(labels_test[:, 1] == str(snr))\n",
    "    mods = ['AM-SSB', 'PAM4', 'QPSK', '8PSK', 'BPSK', 'QAM16', 'QAM64', 'WBFM', 'CPFSK', 'AM-DSB', 'GFSK']\n",
    "    \n",
    "    loss_and_metrics = model.evaluate(X_test[idx], y_test[idx], batch_size=128);\n",
    "    metrics_test.append((snr, loss_and_metrics[0], loss_and_metrics[1]))\n",
    "\n",
    "df_metrics_test = pd.DataFrame(metrics_test, columns=['snr', 'loss', 'acc'])\n",
    "df_metrics_test.plot(kind='scatter', x='snr', y='acc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
